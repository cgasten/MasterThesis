{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb2f902-5914-480c-8165-917f466013bd",
   "metadata": {},
   "source": [
    "# Drought Indicator Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af5b90-b3e4-45f7-adda-34e12a7c4689",
   "metadata": {},
   "source": [
    "@author: Caroline Gasten\n",
    "\n",
    "The present notebook caculates the SPI and SPEI per grid cell and then aggregates these DIs to the admin1-level and ethnic group territory level by applying different statistics (10th, 25th, 75th, 90th percentile, median and mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977eb6d-43e5-4422-b748-1128a458e815",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527dcf3-5833-4201-ba14-8af9d7639208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma, norm, fisk\n",
    "import datetime as dt\n",
    "from datetime_periods import period\n",
    "import geopandas as gpd\n",
    "from scipy.special import binom\n",
    "from scipy.special import gamma as gam\n",
    "import progressbar\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203816b-e9cc-4ae6-b8f5-299697c24eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "path_prec_input = #path to daily precipitation data\n",
    "path_pet_input = #path to PET data\n",
    "path_out = #path where gridded DI data shall be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842af40-704a-4e22-9ce7-7f138418e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fitting time period and calculation time period\n",
    "time_fit = ['1993', '2023'] # for fitting the distribution a 30 year period is used from 1993 and 2023\n",
    "time_calc = ['2002', '2023'] # SPI and SPEI are calculated from 2002 - 2023 (2 years prior to the period of investigation to account for aggregation periods and lags needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79eb246-16b3-4fb4-bd86-b0e203cbd80d",
   "metadata": {},
   "source": [
    "## Aggregation to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d8fb5-c7c1-46cd-adff-d3b83d5f2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for year in range(1993, 2023):\n",
    "    #opening files\n",
    "    file_tp = os.path.join(path_prec_input, 'era5_tp_%04d_daily.nc'%(year))\n",
    "    file_pet = os.path.join(path_pet_input, 'PET_%04d_daily.nc'%(year))\n",
    "    ds_tp_d = xr.open_dataarray(file_tp).rename({'x':'longitude', 'y':'latitude'}) * 10**3 #mm/day\n",
    "    ds_pet_d = xr.open_dataarray(file_pet).rename({'x':'longitude', 'y':'latitude'}) #mm/day\n",
    "    \n",
    "    #resampling to monthly time step\n",
    "    ds_tp_m_year = ds_tp_d.resample(time='M').sum()\n",
    "    ds_pet_m_year = ds_pet_d.resample(time='M').sum(skipna=False)\n",
    "    \n",
    "    #concatenating yearly time series to one single time series\n",
    "    if i == 0:\n",
    "        ds_tp_m = ds_tp_m_year\n",
    "        ds_pet_m = ds_pet_m_year\n",
    "    else:\n",
    "        ds_tp_m = xr.concat([ds_tp_m, ds_tp_m_year], dim='time')\n",
    "        ds_pet_m = xr.concat([ds_pet_m, ds_pet_m_year], dim='time')\n",
    "        \n",
    "    i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaeb2e4-b902-44ea-9f3e-8286013e58fe",
   "metadata": {},
   "source": [
    "## Calculation of SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd132d-4cf1-4712-bb6e-8daa69aa209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spi_calc(ds_tp_monthly, n_months, time_fit, time_calc):\n",
    "    \"\"\"\n",
    "    The present function aggregates the input dataset to the aggregation time period of interest (defined by n_months) and fits a gamma distribution based on the\n",
    "    time period specified in time_fit. Then, it calculates the SPI values for the time period specified in time_calc.\n",
    "    \"\"\"\n",
    "    # aggregation of monthly precipitation values based on aggregation time period\n",
    "    ds_tp = ds_tp_monthly.rolling(time=n_months, min_periods=n_months).sum()\n",
    "    \n",
    "    # slicing datasets according to defined time periods of interest for fitting and calculating SPI values\n",
    "    ds_fit = ds_tp.sel(time=slice(time_fit[0], time_fit[1]))\n",
    "    ds_calc = ds_tp.sel(time=slice(time_calc[0], time_calc[1]))\n",
    "    \n",
    "    #set up xr.Dataset to store spi values in the same format as precipitation values\n",
    "    xr_shape = [len(ds_calc.time), len(ds_calc.latitude), len(ds_calc.longitude)]\n",
    "    spi_ds = xr.DataArray(np.empty(xr_shape),coords = dict(time = ds_calc.time, latitude=ds_calc.latitude,longitude=ds_calc.longitude), attrs = dict(unit='-', long_name=f'Standardized Precipitation Index {n_months} months'))\n",
    "    \n",
    "    #progress bar visualization\n",
    "    n_calcs = len(ds_calc.longitude) * len(ds_calc.latitude) * 12\n",
    "    bar = progressbar.ProgressBar(maxval=n_calcs)\n",
    "    i=0\n",
    "    bar.start()\n",
    "    \n",
    "    #fitting and calculating SPI values by separately fitting a gamma distribution for each grid cell and month of the year\n",
    "    for lon in ds_tp.longitude:\n",
    "        for lat in ds_tp.latitude:\n",
    "            \n",
    "            #selecting grid cell of interest\n",
    "            df_fit = ds_fit.sel(longitude=lon, latitude=lat).to_dataframe().tp\n",
    "            df_calc = ds_calc.sel(longitude=lon, latitude=lat).to_dataframe().tp\n",
    "            for m in df_fit.index.month.unique():\n",
    "                i  += 1\n",
    "                bar.update(i)\n",
    "                #selecting month of interest\n",
    "                df_fit_m = df_fit[df_fit.index.month==m]\n",
    "                df_calc_m = df_calc[df_calc.index.month==m]\n",
    "                \n",
    "                #fitting SPI values and calculating them for latxlonxmonth combination and writing it to a datarray which is iteratively filled with values for all grid cells and months\n",
    "                spi_m = spi_for_month(df_fit_m, df_calc_m)\n",
    "                spi_ds.loc[dict(time=spi_m.index, latitude=lat, longitude=lon)] = xr.DataArray(spi_m)\n",
    "    bar.finish()          \n",
    "    return spi_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db853488-e52a-4760-bf56-da217f31ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spi_for_month(df_fit_m, df_calc_m):\n",
    "    \"\"\"\n",
    "    The present function receives as input the dataframe of precipitation values for a certain aggregation time period, grid cell and month of the year \n",
    "    (over all years in question) and fits the gamma distribution to the values provided in df_fit_m. Based on the fitted distribution it calculates the \n",
    "    SPI values for the precipitation values in df_calc_m.\n",
    "    \"\"\"\n",
    "    #remove zero values: gamma distribution not defined for zero, it is expected that in the arid environment especially 1month and 3month aggregation time periods may equal zero. Therefore, the mixed distribution approach needs to be used. see paper by European Commission\n",
    "    df_fit_nonzero = df_fit_m[df_fit_m > 0]\n",
    "    \n",
    "    #probability of zero \n",
    "    n_samples = len(df_fit_m)\n",
    "    n_zero = n_samples - len(df_fit_nonzero)\n",
    "    prob_zero = n_zero/n_samples\n",
    "    \n",
    "    #fit gamma distribution\n",
    "    a, loc, scale = gamma.fit(df_fit_nonzero, floc=0) #max likelihood method\n",
    "    \n",
    "    #cumulative distribution function for calculation time period to derive SPI values\n",
    "    cdf_nonzero = gamma.cdf(df_calc_m[df_calc_m>0], a=a, loc=loc, scale=scale)\n",
    "    cdf = pd.DataFrame(data=np.ones(len(df_calc_m))*prob_zero, index=df_calc_m.index, columns=['P_cum']) #probability assigned to precipitation values of 0 (or lower)\n",
    "    cdf[df_calc_m>0] = cdf[df_calc_m>0] + (1 - prob_zero)*cdf_nonzero.reshape(len(cdf_nonzero), 1) #adding probability based on nonzero cdf to all other precipitation values\n",
    "    spi = pd.Series(norm.ppf(cdf.P_cum), index = cdf.index) #SPI as values which correspond to the values resulting in the probabilities of the normalized cdf function\n",
    "    \n",
    "    return spi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebf532-da47-4ec8-b9f8-067d2161ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPI-1\n",
    "spi_1 = spi_calc(ds_tp_m, 1, time_fit, time_calc)\n",
    "spi_1.to_netcdf(os.path.join(path_out,'SPI', 'SPI-1.nc'))\n",
    "#problem when there are no really positive values -> cannot fit a distribution -> nan values but only limited to very few cells and months of the year, for statistical analysis at larger level these values are not considered to play a large difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37988890-0f83-4e70-88b9-5b7f75865e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a0024-2d16-4624-a0e8-ab59e5ba3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPI-3\n",
    "spi_3 = spi_calc(ds_tp_m, 3, time_fit, time_calc)\n",
    "spi_3.to_netcdf(os.path.join(path_out,'SPI', 'SPI-3.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210b0e6-96bd-46d1-840d-1cc5de6d5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55881452-93a0-44a1-814c-cd18968c3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPI-6\n",
    "spi_6 = spi_calc(ds_tp_m, 6, time_fit, time_calc)\n",
    "spi_6.to_netcdf(os.path.join(path_out,'SPI', 'SPI-6.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707d12f-caa1-41d2-a7fe-28199fbd67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_6.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1682b-f747-49bf-a9ed-f72561555738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPI-12\n",
    "spi_12 = spi_calc(ds_tp_m, 12, time_fit, time_calc)\n",
    "spi_12.to_netcdf(os.path.join(path_out,'SPI', 'SPI-12.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fab64-12a1-4d0b-9026-f6a5b718ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_12.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16e11e-ba03-4d70-a5d1-30a1586fc7df",
   "metadata": {},
   "source": [
    "## SPEI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280b8b1-e54d-4601-952a-d5108ef2e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spei_calc(ds_tp_monthly, ds_pet_monthly, n_months, time_fit, time_calc):\n",
    "    \"\"\"\n",
    "    The present function calculates the SPEI for each grid cell and month based on a ... distribution which is fitted for each latxlonxmonth-of-the-year \n",
    "    combination of the difference between n_months aggregation in precipitation and evapotranspiration over the time period specified in time_fit and \n",
    "    which is then used to calculate the SPEI values for the same latxlonxmonth-of-the-year combination but for the time period specified in time_calc.\n",
    "    \"\"\"\n",
    "    #aggregation of TP-PET over the number of months specified in n_months\n",
    "    ds_diff = ds_tp_monthly.rolling(time=n_months, min_periods=n_months).sum() - ds_pet_monthly.rolling(time=n_months, min_periods=n_months).sum().dropna(dim='time')\n",
    "    ds_diff = ds_diff.rename('wb')\n",
    "    \n",
    "    #select the time periods to use for fitting and for calculation from the newly created dataset\n",
    "    ds_fit = ds_diff.sel(time=slice(time_fit[0], time_fit[1]))\n",
    "    ds_calc = ds_diff.sel(time=slice(time_calc[0], time_calc[1]))\n",
    "\n",
    "    #set up xr.Dataarray to store spei values in the same format as precipitation and pet values\n",
    "    xr_shape = [len(ds_calc.time), len(ds_calc.latitude), len(ds_calc.longitude)]\n",
    "    spei_ds = xr.DataArray(np.zeros(xr_shape),coords = dict(time = ds_calc.time, latitude=ds_calc.latitude,longitude=ds_calc.longitude), attrs = dict(unit='-', long_name=f'Standardized Precipitation Evapotranspiration Index {n_months} months'))\n",
    "    \n",
    "    #progress bar visualization\n",
    "    n_calcs = len(ds_calc.longitude) * len(ds_calc.latitude) * 12\n",
    "    bar = progressbar.ProgressBar(maxval=n_calcs)\n",
    "    i=0\n",
    "    bar.start()\n",
    "    \n",
    "    #fitting and calculating SPEI values by separately fitting a log-logistic distribution for each grid cell and month of the year\n",
    "    for lon in ds_calc.longitude:\n",
    "        for lat in ds_calc.latitude:\n",
    "            #selecting grid cell of interst\n",
    "            df_fit = ds_fit.sel(longitude=lon, latitude=lat).to_dataframe().wb\n",
    "            df_calc = ds_calc.sel(longitude=lon, latitude=lat).to_dataframe().wb\n",
    "            for m in df_fit.index.month.unique():\n",
    "                i  += 1\n",
    "                bar.update(i)\n",
    "                #selecting month of interest\n",
    "                df_fit_m = df_fit[df_fit.index.month==m]\n",
    "                df_calc_m = df_calc[df_calc.index.month==m]\n",
    "                #fitting SPEI values and calculating them for latxlonxmonth combination and writing it to a datarray which is iteratively filled with values for all grid cells and months\n",
    "                spei_m = spei_for_month(df_fit_m, df_calc_m)\n",
    "                if spei_m.isnull().sum()>0:\n",
    "                    print(f'null value at longitude {lon}, latitude{lat} and month {m}')\n",
    "                spei_ds.loc[dict(time=spei_m.index, latitude=lat, longitude=lon)] = xr.DataArray(spei_m)\n",
    "    bar.finish()\n",
    "    return spei_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab35c3-d01e-4816-843b-ad08897dd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spei_for_month(df_fit_m, df_calc_m):\n",
    "    # fit log-logistic distribution using unbiased Probability Weighted Moments (PWM) -> based on Vicente-Serrano for parameter calculation, Begueria for using unbiased and Hosking 1986 for the formula for the PWM\n",
    "    \n",
    "    df_fit_m_sorted= df_fit_m.sort_values()\n",
    "    \n",
    "    #calculate unbiased PWM 0 to 2\n",
    "    w_0=0\n",
    "    w_1=0\n",
    "    w_2=0\n",
    "    i=0\n",
    "    n = len(df_fit_m_sorted)\n",
    "    for val in df_fit_m_sorted:\n",
    "        i+=1\n",
    "        w_0 += 1/n * binom(n-i, 0) * val/binom(n-1, 0)\n",
    "        w_1 += 1/n * binom(n-i, 1) * val/binom(n-1, 1)\n",
    "        w_2 += 1/n * binom(n-i, 2) * val/binom(n-1, 2)\n",
    "    \n",
    "    #calculate scale, shape and loc parameter for 3-parameter log-logistic distribution\n",
    "    beta = (2*w_1 - w_0) / (6*w_1 - w_0 - 6*w_2) #shape\n",
    "    alpha = (w_0 - 2 * w_1) * beta / (gam(1 + 1/beta) * gam(1 - 1/beta)) #scale\n",
    "    gamma = w_0 - alpha * gam(1 + 1/beta) * gam(1 - 1/beta) #loc\n",
    "    \n",
    "    \n",
    "    #obtain distribution for time period of interest\n",
    "    prob_df =pd.DataFrame()\n",
    "    prob_df['P-PET']=df_calc_m\n",
    "    prob_df['pdf'] = beta / alpha * ((prob_df['P-PET'] - gamma)/alpha)**(beta - 1) * (1 + ((prob_df['P-PET'] - gamma)/alpha)**beta)**(-2)#fisk.pdf(prob_df['P-PET'], beta, loc=gamma, scale=alpha)\n",
    "    prob_df['Fx'] = (1 + ((prob_df['P-PET'] - gamma)/alpha)**(-beta))**(-1)#fisk.cdf(prob_df['P-PET'], beta, loc=gamma, scale=alpha)\n",
    "\n",
    "  \n",
    "    #setting cumulative probability for values below origin to 0\n",
    "    if prob_df.Fx.isnull().any()==1:\n",
    "        df_debug = prob_df[prob_df.Fx.isnull()]\n",
    "        for idx in df_debug.index:\n",
    "            if (df_debug.loc[idx, 'P-PET'] - gamma)<0 and alpha>0:\n",
    "                prob_df.loc[idx, 'Fx'] = 0\n",
    "                prob_df.loc[idx, 'pdf'] = 0\n",
    "    \n",
    "                \n",
    "    #standardize following procedure in Vicente-Serrano et al 2010 based on Abramowitz Stegun\n",
    "    prob_df['P_ex'] = 1 - prob_df.Fx\n",
    "    prob_df['SPEI'] = np.ones(len(prob_df))\n",
    "    prob_df.loc[prob_df.P_ex>0.5, 'SPEI'] = -1\n",
    "    prob_df.loc[prob_df.P_ex>0.5, 'P_ex'] = 1 - prob_df.loc[prob_df.P_ex>0.5, 'P_ex']\n",
    "    prob_df.P_ex.where(prob_df.P_ex!=0, 10**(-10), inplace=True) #avoid that P=0\n",
    "    prob_df['W'] = np.sqrt(-2 * np.log(prob_df.P_ex))\n",
    "    C_0 = 2.515517\n",
    "    C_1 = 0.802853\n",
    "    C_2 = 0.010328\n",
    "    d_1 = 1.432788\n",
    "    d_2 = 0.189269\n",
    "    d_3 = 0.001308\n",
    "    prob_df['SPEI'] = prob_df.SPEI * (prob_df.W - (C_0 + C_1 * prob_df.W + C_2 * prob_df.W**2)/(1+d_1*prob_df.W + d_2 * prob_df.W**2 + d_3 * prob_df.W**3))\n",
    "    #prob_df.loc[prob_df.SPEI<-4, 'SPEI'] = -4\n",
    "    return prob_df.SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa2929-3ac1-435c-8f1c-fdb51fb326c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPEI-1\n",
    "spei_1 = spei_calc(ds_tp_m, ds_pet_m, 1, time_fit, time_calc)\n",
    "spei_1.to_netcdf(os.path.join(path_out, 'SPEI', 'SPEI-1.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d45c1-5eb0-431f-a09e-4174ec195779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SPEI-3\n",
    "spei_3 = spei_calc(ds_tp_m, ds_pet_m, 3, time_fit, time_calc)\n",
    "spei_3.to_netcdf(os.path.join(path_out, 'SPEI', 'SPEI-3.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb637871-544f-4e9e-be2a-24da4ccf3b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SPEI-6\n",
    "spei_6 = spei_calc(ds_tp_m, ds_pet_m, 6, time_fit, time_calc)\n",
    "spei_6.to_netcdf(os.path.join(path_out, 'SPEI', 'SPEI-6.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235fddf-452d-4ac3-a9a9-cae3faf8ae7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SPEI-12\n",
    "spei_12 = spei_calc(ds_tp_m, ds_pet_m, 12, time_fit, time_calc)\n",
    "spei_12.to_netcdf(os.path.join(path_out, 'SPEI', 'SPEI-12.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac046a-f597-4556-a508-a7244c34e4da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SPI & SPEI per adm-1 unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d18a57-4b4f-40c3-bbec-2d058f3ff009",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db9464-2a4f-43f0-b277-c18fc8291d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "import math as mt\n",
    "from geocube.api.core import make_geocube\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736040d3-ae3b-4b4b-9e89-3349dfaeb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prec_input = #path to daily precipitation data\n",
    "path_pet_input = #path to daily PET data\n",
    "path_out = #path where spatial statistics shall be stored\n",
    "path_adm = #path to administrative unit shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d4bff-010e-44eb-b06c-24956136d607",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf2738-5bd2-482d-9b01-09f6a0937a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_shape(drought_xds, mask_shapefile, crs='epsg:4326',  incl_all_touching=True):\n",
    "    \"\"\"\n",
    "    clips drought dataset based on mask extents\n",
    "    drought_xds: xarray Dataset containing the spi or spei values along dimensions time, latitude and longitude\n",
    "    crs: string of the coordinate reference system in EPSG or WSG format\n",
    "    mask_shapefile: mask shapefile loaded into python\n",
    "    incl_all_touching: boolean which indicates if all cells touching the border should be included even if their centroid lies outside\n",
    "    \"\"\"\n",
    "    drought_xds.rio.write_crs(crs, inplace=True)\n",
    "    drought_clipped = drought_xds.rio.clip(mask_shapefile.geometry, all_touched=incl_all_touching, from_disk=True)\n",
    "    return drought_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97948847-220d-4d82-b289-c9c0da7ae6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_by_adm(gadm_file, adm_name, DI_name, country, crs='epsg:4326', incl_all_touching=True):\n",
    "    \"\"\"\n",
    "    function calculates the spatial statistics of drought indicator over admin unit\n",
    "    gadm_file: shapefile of the admin unit\n",
    "    adm_name: admin-unit name\n",
    "    DI_name: name of the drought indicator\n",
    "    crs: coordinate reference system of the drought raster dataset, default epsg:4326\n",
    "    incl_all_touching: method to use to cut out values of drought indicators\n",
    "    \"\"\"\n",
    "    #open drought dataset\n",
    "    if 'SPI' in DI_name:\n",
    "        DI = xr.open_dataarray(os.path.join(path_out, 'SPI', '%s.nc'%DI_name)).rename('spi')\n",
    "    elif 'SPEI' in DI_name:\n",
    "        DI = xr.open_dataarray(os.path.join(path_out, 'SPEI', '%s.nc'%DI_name)).rename('spei')\n",
    "        \n",
    "    #retrieve admin-unit from country dataframe of admin1-units\n",
    "    mask = gadm_file[gadm_file.index == adm_name]\n",
    "    \n",
    "    #clip drought data and save to dataset\n",
    "    DI_county = clip_to_shape(DI, mask, crs, incl_all_touching)\n",
    "    if 'SPI' in DI_name:\n",
    "        DI_county.to_netcdf(os.path.join(path_out, 'SPI', 'clipped', '%s-%s.nc'%(DI_name, adm_name)))\n",
    "    elif 'SPEI' in DI_name:\n",
    "        DI_county.to_netcdf(os.path.join(path_out, 'SPEI', 'clipped', '%s-%s.nc'%(DI_name, adm_name)))\n",
    "        \n",
    "    #calculate spatial statistics    \n",
    "    df_di_county = pd.DataFrame(DI_county.time.values, columns=['time']).set_index('time')\n",
    "    df_di_county['Mean'] = DI_county.mean(dim=['longitude', 'latitude'])\n",
    "    df_di_county['Median'] = DI_county.median(dim=['longitude','latitude'])\n",
    "    df_di_county['Min'] = DI_county.min(dim=['longitude', 'latitude'])\n",
    "    df_di_county['Max'] = DI_county.max(dim=['longitude', 'latitude'])\n",
    "    df_di_county['P10'] = DI_county.quantile(0.1, dim=['longitude', 'latitude'])\n",
    "    df_di_county['P25'] = DI_county.quantile(0.25, dim=['longitude', 'latitude'])\n",
    "    df_di_county['Median'] = DI_county.quantile(0.5, dim=['longitude', 'latitude'])\n",
    "    df_di_county['P75'] = DI_county.quantile(0.75, dim=['longitude', 'latitude'])\n",
    "    df_di_county['P90'] = DI_county.quantile(0.9, dim=['longitude', 'latitude'])\n",
    "    df_di_county['Std'] = DI_county.std(dim=['longitude', 'latitude'])\n",
    "    \n",
    "    #save spatial statistics\n",
    "    if 'SPI' in DI_name:\n",
    "        df_di_county.to_csv(os.path.join(path_out, 'SPI', 'stats', '%s-%s-%s.csv'%(country, DI_name, adm_name)))\n",
    "    elif 'SPEI' in DI_name:\n",
    "        df_di_county.to_csv(os.path.join(path_out, 'SPEI', 'stats', '%s-%s-%s.csv'%(country, DI_name, adm_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56301b54-0974-47dd-8836-7db56065c975",
   "metadata": {},
   "source": [
    "### Statistics Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c0e23-7ce3-4e17-aea8-e62ee51bd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Kenya, Somalia, South Sudan and Uganda\n",
    "for country in ['KEN', 'UGA', 'SOM', 'SSD']:\n",
    "    \n",
    "    #reading and dissolving GADM 3.6 shapefiles to admin-1 level\n",
    "    adm_bound = gpd.read_file(os.path.join(path_adm, country, 'gadm36_%s.gpkg'%(country)))\n",
    "    adm1 = adm_bound.dissolve(by='NAME_1').loc[:, ['GID_1', 'geometry']]\n",
    "    \n",
    "    #progress bar visualization\n",
    "    print(country)\n",
    "    n_calcs = 8 * len(adm1)\n",
    "    bar = progressbar.ProgressBar(maxval=n_calcs)\n",
    "    i=0\n",
    "    bar.start()\n",
    "    \n",
    "    #calculation of statistics for each DI per month and administrative unit\n",
    "    for di in ['SPI-1', 'SPI-3', 'SPI-6', 'SPI-12', 'SPEI-1', 'SPEI-3', 'SPEI-6', 'SPEI-12']:\n",
    "        for county in np.unique(adm1.index):\n",
    "            stats_by_adm(adm1, county, di, country)\n",
    "            i+=1\n",
    "            bar.update(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc651ac4-4f93-42c3-be1a-36d2937dec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Ethiopia because of different file strucutre\n",
    "\n",
    "#reading admin-1 level shapefiles as for Ethiopia saved differently\n",
    "country = 'ETH'\n",
    "adm1 = gpd.read_file(os.path.join(path_adm, country, 'gadm36_%s_1.shp'%(country))).loc[:, ['NAME_1', 'GID_1', 'geometry']].set_index('NAME_1')\n",
    "\n",
    "#visualization of progress bar\n",
    "n_calcs = 8 * len(adm1)\n",
    "bar = progressbar.ProgressBar(maxval=n_calcs)\n",
    "i=0\n",
    "bar.start()\n",
    "\n",
    "#calculation of statistics for each DI per month\n",
    "for di in ['SPI-1', 'SPI-3', 'SPI-6', 'SPI-12', 'SPEI-1', 'SPEI-3', 'SPEI-6', 'SPEI-12']:\n",
    "    for county in np.unique(adm1.index):\n",
    "        stats_by_adm(adm1, county, di, country)\n",
    "        i+=1\n",
    "        bar.update(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d06b4e-25ab-4006-af4e-72bd0441453b",
   "metadata": {},
   "source": [
    "## SPI and SPEI for ethnic group territory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631efa8-da95-4853-936a-d16a3a947453",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e598b-78a6-4d90-befe-3dc4641aa5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_eth = #path to ethnic group territory shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d718f-f044-431b-b8eb-29b19e4a2987",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3599a-a6c4-4dec-a0ea-93f05c937d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_by_eth(eth_file, eth_name, DI_name, crs='epsg:4326', incl_all_touching=True):\n",
    "      \n",
    "    \"\"\"\n",
    "    function calculates the spatial statistics of drought indicator over ethnic group level\n",
    "    eth_file: shapefile of the ethnic group territory\n",
    "    eth_name: ethnic group name\n",
    "    DI_name: name of the drought indicator\n",
    "    crs: coordinate reference system of the drought raster dataset, default epsg:4326\n",
    "    incl_all_touching: method to use to cut out values of drought indicators\n",
    "    \"\"\"\n",
    "    \n",
    "    #open drought dataset\n",
    "    if 'SPI' in DI_name:\n",
    "        DI = rioxarray.open_rasterio(os.path.join(path_out, 'SPI', '%s.nc'%DI_name)).rename('spi')\n",
    "    elif 'SPEI' in DI_name:\n",
    "        DI = rioxarray.open_rasterio(os.path.join(path_out, 'SPEI', '%s.nc'%DI_name)).rename('spei')\n",
    "        \n",
    "    #clip drought dataset to ethnic group territory shape\n",
    "    mask = eth_file\n",
    "    DI_eth = clip_to_shape(DI, mask, crs, incl_all_touching)\n",
    "    if 'SPI' in DI_name:\n",
    "        DI_eth.to_netcdf(os.path.join(path_out, 'SPI', 'clipped', 'Eth_%s-%s.nc'%(DI_name, eth_name)))\n",
    "    elif 'SPEI' in DI_name:\n",
    "        DI_eth.to_netcdf(os.path.join(path_out, 'SPEI', 'clipped', 'Eth_%s-%s.nc'%(DI_name, eth_name)))                                                        \n",
    "    \n",
    "    df_di_eth = pd.DataFrame(DI_eth.time.values, columns=['time']).set_index('time')\n",
    "    \n",
    "    #calculate spatial statistics\n",
    "    df_di_eth['Mean'] = DI_eth.mean(dim=['x', 'y'])\n",
    "    df_di_eth['Median'] = DI_eth.median(dim=['x','y'])\n",
    "    df_di_eth['Min'] = DI_eth.min(dim=['x', 'y'])\n",
    "    df_di_eth['Max'] = DI_eth.max(dim=['x', 'y'])\n",
    "    df_di_eth['P10'] = DI_eth.quantile(0.1, dim=['x', 'y'])\n",
    "    df_di_eth['P25'] = DI_eth.quantile(0.25, dim=['x', 'y'])\n",
    "    df_di_eth['Median'] = DI_eth.quantile(0.5, dim=['x', 'y'])\n",
    "    df_di_eth['P75'] = DI_eth.quantile(0.75, dim=['x', 'y'])\n",
    "    df_di_eth['P90'] = DI_eth.quantile(0.9, dim=['x', 'y'])\n",
    "    df_di_eth['Std'] = DI_eth.std(dim=['x', 'y'])\n",
    "    \n",
    "    #save output datafiles\n",
    "    if 'SPI' in DI_name:\n",
    "        df_di_eth.to_csv(os.path.join(path_out, 'SPI', 'stats','Ethnic_group',  'Eth_%s-%s.csv'%(DI_name, eth_name)))\n",
    "    elif 'SPEI' in DI_name:\n",
    "        df_di_eth.to_csv(os.path.join(path_out, 'SPEI', 'stats', 'Ethnic_group', 'Eth_%s-%s.csv'%(DI_name, eth_name)))\n",
    "    \n",
    "    return df_di_eth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a959c1d-dc9c-42f3-9c1c-e702e7f788d4",
   "metadata": {},
   "source": [
    "### Statistics Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38a6b9-80cc-4a1a-ae52-4c0f517be4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#names of ethnic groups under investigation\n",
    "eth_groups = [\"Toposa\", \"Dassanetch\", \"Pokot\", \"Borana\", \"Gabra\", \"Turkana\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b9f19-e4b4-46aa-9c9e-c2ca5ff11b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#progress bar\n",
    "n_calcs = 8 * len(eth_groups)\n",
    "bar = progressbar.ProgressBar(maxval=n_calcs)\n",
    "i=0\n",
    "bar.start()\n",
    "\n",
    "#loop through ethnic groups and DIs to calculate spatial statistics of each DI over respective territory\n",
    "for eth in eth_groups:\n",
    "    eth_shape = gpd.read_file(os.path.join(path_eth, 'Eth_%s.shp'%(eth)))\n",
    "    for di in ['SPI-1', 'SPI-3', 'SPI-6', 'SPI-12', 'SPEI-1', 'SPEI-3', 'SPEI-6', 'SPEI-12']:\n",
    "        i+=1\n",
    "        bar.update(i)\n",
    "        DI_eth = stats_by_eth(eth_shape, eth, di)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
